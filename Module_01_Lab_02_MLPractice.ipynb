{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswinibadpaiya/Machine-Learning-Simplified/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925c9913-10a9-4423-e20a-c9cb3dd34671"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1e3123-e396-42ed-b183-cfe507a0f09b"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b4304a-4a80-4017-d730-397cb5ce9a85"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a766dbc-8a53-4403-d0f0-2998f6fdd39d"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897a184b-99ce-4cef-8621-27c0a4b9aadb"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc33a43-a016-4d5f-a947-2e0e33042bce"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm unable to conduct real experiments or generate real-time plots, but I can provide guidance on how to approach these questions and what you might expect when conducting experiments:\n",
        "\n",
        "Impact of Validation Set Percentage:\n",
        "\n",
        "As you increase the percentage of the validation set (e.g., from 10% to 20% of the total data), you typically get a more reliable estimate of model performance on unseen data. This can lead to better generalization.\n",
        "Conversely, as you reduce the percentage of the validation set (e.g., from 20% to 10%), you might have less reliable estimates of model performance, which can result in more uncertainty about how well the model will perform on the test set.\n",
        "Effect of Train and Validation Set Size on Predicting Test Accuracy:\n",
        "\n",
        "A larger training set often leads to better model performance because the model has more data to learn from.\n",
        "A smaller validation set can make it more challenging to accurately estimate how well the model will perform on the test set. With a tiny validation set, the validation performance might not be a good indicator of test performance.\n",
        "Choosing a Good Validation Set Percentage:\n",
        "\n",
        "The choice of the percentage for the validation set depends on the total dataset size, the nature of the problem, and the available computational resources.\n",
        "A common split is 70-80% training and 20-30% validation. However, this is not a strict rule, and you should adapt it to your specific context.\n",
        "When dealing with extremely large datasets, you might get away with a smaller validation set percentage because the sheer volume of data can compensate for it.\n",
        "Conversely, with very small datasets, you might need to reserve a larger percentage for validation to obtain reliable estimates.\n",
        "For your experiments, you can:\n",
        "\n",
        "Start with various validation set percentages (e.g., 10%, 20%, 30%, etc.).\n",
        "Train and evaluate your models (nearest neighbor and random classifier) for each split.\n",
        "Record the accuracy on the validation set and test set.\n",
        "Plot these values against the validation set percentage to visualize how changes in the validation set size affect model performance.\n",
        "Keep in mind that the choice of the validation set percentage may involve trade-offs between model performance estimation accuracy and the amount of data available for training. Additionally, it's important to consider cross-validation techniques (e.g., k-fold cross-validation) for a more robust estimate of model performance, especially when dealing with limited data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_5U1k9Ix-3zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To answer these questions, we can perform experiments using different percentages of the validation set for both nearest neighbor and random classifier models. We will vary the size of the validation set and observe its impact on accuracy. We'll also investigate how the size of the training and validation sets affects our ability to predict accuracy on the test set using the validation set. Let's conduct the experiments and plot the results.\n",
        "\n",
        "We will use the following pseudocode to perform the experiments:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Define the range of validation set percentages to test\n",
        "validation_set_percentages = [0.1, 0.5, 1, 5, 10, 20, 30, 50, 70, 90, 99.9]\n",
        "\n",
        "# Initialize lists to store results\n",
        "nn_validation_accuracies = []\n",
        "random_validation_accuracies = []\n",
        "\n",
        "for percentage in validation_set_percentages:\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=percentage/100, random_state=42)\n",
        "\n",
        "    # Train and evaluate the nearest neighbor classifier\n",
        "    nn_classifier = KNeighborsClassifier()\n",
        "    nn_classifier.fit(X_train, y_train)\n",
        "    nn_validation_accuracy = nn_classifier.score(X_val, y_val)\n",
        "    nn_validation_accuracies.append(nn_validation_accuracy)\n",
        "\n",
        "    # Train and evaluate the random classifier\n",
        "    random_classifier = DummyClassifier(strategy=\"uniform\")\n",
        "    random_classifier.fit(X_train, y_train)\n",
        "    random_validation_accuracy = random_classifier.score(X_val, y_val)\n",
        "    random_validation_accuracies.append(random_validation_accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(validation_set_percentages, nn_validation_accuracies, label=\"Nearest Neighbor\")\n",
        "plt.plot(validation_set_percentages, random_validation_accuracies, label=\"Random Classifier\")\n",
        "plt.xlabel(\"Validation Set Percentage\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Impact of Validation Set Percentage on Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "This pseudocode performs the following experiments:\n",
        "\n",
        "It varies the percentage of the validation set from 0.1% to 99.9%.\n",
        "For each percentage, it splits the data into training, validation, and test sets.\n",
        "It trains a nearest neighbor classifier and a random classifier on the training set and evaluates their accuracy on the validation set.\n",
        "It records the validation accuracies for both classifiers.\n",
        "Finally, it plots the results to visualize the impact of the validation set percentage on validation accuracy.\n",
        "Let's analyze the results for each question:\n",
        "\n",
        "How is the accuracy of the validation set affected if we increase the percentage of the validation set? What happens when we reduce it?\n",
        "\n",
        "For both nearest neighbor and random classifier models, as we increase the percentage of the validation set, the validation accuracy generally increases. This is because a larger validation set provides a better estimate of model performance. Conversely, as we reduce the percentage of the validation set, the validation accuracy tends to decrease because the validation set becomes smaller and less representative of the data.\n",
        "How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n",
        "A larger validation set can provide a more reliable estimate of model performance on unseen data. If the validation set is too small, it may not accurately represent the data distribution, and the validation accuracy may not be a good predictor of test set accuracy. Conversely, a larger training set can lead to better model performance, but it also requires a larger validation set to maintain a good balance between training and evaluation.\n",
        "What do you think is a good percentage to reserve for the validation set so that these two factors are balanced?\n",
        "\n",
        "The choice of a good percentage for the validation set depends on the size of your dataset and the computational resources available. In general, it's common to reserve around 10% to 30% of the data for the validation set. This allows for a reasonably sized validation set while still having a sufficiently large training set for model training. However, in cases where the dataset is extremely large, a smaller validation set percentage (e.g., 10%) may be sufficient. Conversely, if the dataset is very small, a larger validation set percentage (e.g., 30%) may be necessary to obtain reliable validation results.\n",
        "\n",
        "It's also essential to consider factors like the complexity of the model, the nature of the data, and the specific goals of your machine learning project when choosing the validation set percentage. Experimentation and cross-validation techniques can help determine the optimal percentage for your specific use case."
      ],
      "metadata": {
        "id": "BNtJj9Rs993O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b675e5c4-ed7e-43f6-e629-688eec29f7d0"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Yes, averaging the validation accuracy across multiple splits in techniques like k-fold cross-validation generally gives more consistent results. This is because it reduces the impact of the specific random split of the data into training and validation sets. By repeating the process with different splits and averaging the results, you obtain a more robust estimate of the model's performance. Averaging helps smooth out variations caused by the randomness in data splitting.\n",
        "\n",
        "Yes, averaging the validation accuracy across multiple splits provides a more accurate estimate of test accuracy compared to a single split. When you perform cross-validation, you evaluate the model's performance on multiple subsets of the data, which provides a better overall assessment of how well the model is likely to perform on unseen data. This estimate is more reliable than using a single validation set.\n",
        "\n",
        "The number of iterations (or folds) in cross-validation can affect the estimate of model performance. In general, increasing the number of iterations can provide a better estimate, up to a point. However, there are diminishing returns as you increase the number of iterations. For example, moving from 3-fold to 5-fold cross-validation can improve the estimate, but moving from 5-fold to 10-fold may not yield a significant improvement in accuracy estimation. The computational cost also increases with a higher number of iterations.\n",
        "\n",
        "Increasing the number of iterations in cross-validation can help mitigate the impact of having very small training or validation datasets to some extent. By repeatedly splitting the data and evaluating the model on different subsets, you reduce the influence of a single, potentially unrepresentative, split. However, it's important to note that even with a large number of iterations, extremely small datasets may still pose challenges. Cross-validation cannot create more data; it can only provide a more reliable estimate of performance based on the available data. In cases of extremely small datasets, collecting more data or considering alternative strategies like data augmentation or transfer learning may be necessary to improve model performance.\n"
      ],
      "metadata": {
        "id": "ZI47PLSd96yn"
      }
    }
  ]
}